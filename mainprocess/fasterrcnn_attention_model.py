import torch
import torch.nn as nn

# define a basic self-attention mechanism here
# add an attention layer into the model after the original backbone
